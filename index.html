<!DOCTYPE html>
<html lang="en">
    <head>
        <title>Xiang Lisa Li </title>
        <meta http-equiv="content-type" content="text/html; charset=UTF-8">
        <meta charset="utf-8">
	    <meta property="og:title" content="Xiang Li" />
	    <meta property="og:image" content="https://xiangli1999.github.io/img/lisa.jpg" />
	    <meta http-equiv="X-UA-Compatible" content="IE=edge">
	    <meta name="author" content="Xiang Li">
        <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
        <link rel="shortcut icon" type="image/png" href="favicon.ico"/>

        <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css" integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous">
        <link rel="stylesheet" href="css/style.css">
        <script src="https://code.jquery.com/jquery-3.3.1.slim.min.js" integrity="sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo" crossorigin="anonymous"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.7/umd/popper.min.js" integrity="sha384-UO2eT0CpHqdSJQ6hJty5KVphtPhzWj9WO1clHTMGa3JDZwrnQq4sF86dIHNDz0W1" crossorigin="anonymous"></script>
        <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/js/bootstrap.min.js" integrity="sha384-JjSmVgyd0p3pXB1rRibZUAYoIIy6OrQ6VrjIEaFf/nJGzIxFDsf4x0xIM+B07jRM" crossorigin="anonymous"></script>
    </head>
    <body>
    	<style>
		pre {
    		text-align: left;
    		white-space: pre-line;
  		}
		</style>
        <div class="container mt-5">
            <div class="row mb-3">
                <div class="col">
                    <h1>Xiang Lisa Li </h1>
                </div>
            </div>
            <div class="row">
                <div class="col-md-4 order-md-2">
                    <img src="img/lisa.jpg" alt="Lisa" class="img-fluid rounded">
                </div>
                <div class="col-md-8 order-md-1">
                    <p>
                        Hi! I am a first year Ph.D. at Stanford University, coadvised by <a href="https://cs.stanford.edu/~pliang" target="_blank">Percy Liang</a> and <a href="https://thashim.github.io/" target="_blank">Tatsunori Hashimoto</a>. I am affiliated with the Stanford NLP group. 

                    </p>
                    <p>
                        I did my undergrad at Johns Hopkins University, majoring in Computer Science and Applied Mathmatics and Statistics. I work on natural language processing, affiliated with the Center for Language and Speech Processing. I am advised by Prof. <a href="http://www.cs.jhu.edu/~jason/" target="_blank">Jason Eisner</a>. I work on structured prediction and syntax. 
                    </p>
                    <p>
                        Previously, I did research at Harvard NLP group, working with Prof. <a href="http://rush-nlp.com" target="_blank">Sasha Rush</a>. My project is about using discrete a latent variable model to aid interpretability and controllability of autoregressive neural network for generation task. 
                    </p>
                </div>
            </div>
            <div class="row">
                <div class="col">
                    <p>
                        Email: xlisali [<a href="https://en.wikipedia.org/wiki/At_sign" target="_blank">at</a>] stanford.edu
                    </p>
                    <p>
                        Links:
                        [<a href="" target="_blank">Full CV</a>] [<a href="https://twitter.com/XiangLisaLi2" target="_blank">Twitter</a>] [<a href="https://github.com/XiangLi1999" target="_blank">Github</a>]
                    </p>
                </div>
            </div>

<!--             <div class="row">
                <div class="col">
                    <h2>Recent News</h2>
                    <ul>
                    	<li>
                            (9/2019) Happy Senior Year !!!
                        </li>
                        <li>
                            (8/2019) Paper on Specializing Word Embeddings (for Parsing) 
                            by Information Bottleneck, accepted to EMNLP 2019.
                        </li>
                        <li>
                            (6/2019) I'm starting a summer intern at Harvardnlp, working on 
                            controllability and interpretability of autoregressive neural 
                            network.
                        </li>
                        <li>
                            (2/2019) Paper on Generative Model of Punctuation
                            for Dependency Parsing, accepted to TACL 2019.
                        </li>
                    </ul>
                </div>
            </div> -->
            
            <hr>
            <div class="row" id="publications">
                <div class="col">
                    <h2>Publications</h2>
                    <ul>
                        <li>

                            <a href="pdf/prefix_tuning.pdf" target="_blank">
                                <b>Prefix-Tuning: Optimizing Continuous Prompts for Generation</b>
                            </a>
                            <br/>
                            <b>Xiang Lisa Li</b>
                            and <a href="https://cs.stanford.edu/~pliang" target="_blank">Percy Liang</a>

                            <br/>
                            In <a href="" target="_blank">
                                <b>
                                    Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers) </b></a>
                            <br/>

                

                            [<a href="#" onclick="$('#acl2021_bib').toggle();return false;">bib</a>]
                            [<a href="#" onclick="$('#acl2021_abstract').toggle();return false;">abstract</a>]
                            <!-- [<a href="" target="_blank">dataset</a>] -->
                            <div id="acl2021_abstract" class="abstract" style="display:none;">
                                <p>
                                    Fine-tuning is the de facto way of leveraging large pretrained language models for downstream tasks. However, fine-tuning modifies all the language model parameters and therefore necessitates storing a full copy for each task. In this paper, we propose prefix-tuning, a lightweight alternative to fine-tuning for natural language generation tasks, which keeps language model parameters frozen and instead optimizes a sequence of continuous task-specific vectors, which we call the prefix. Prefix-tuning draws inspiration from prompting for language models, allowing subsequent tokens to attend to this prefix as if it were ``virtual tokens''. We apply prefix-tuning to GPT-2 for table-to-text generation and to BART for summarization. We show that by learning only 0.1% of the parameters, prefix-tuning obtains comparable performance in the full data setting, outperforms fine-tuning in low-data settings, and extrapolates better to examples with topics that are unseen during training.
                                </p>
                            </div>
                            <div id="acl2021_bib" class="bib" style="display:none;">
                                <pre>
                                @inproceedings{li-liang-2021-prefix,
                                    title = "Prefix-Tuning: Optimizing Continuous Prompts for Generation",
                                    author = "Li, Xiang Lisa  and
                                      Liang, Percy",
                                    booktitle = "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
                                    month = aug,
                                    year = "2021",
                                    address = "Online",
                                    publisher = "Association for Computational Linguistics",
                                    url = "https://aclanthology.org/2021.acl-long.353",
                                    doi = "10.18653/v1/2021.acl-long.353",
                                    pages = "4582--4597",
                                }
                                </pre>
                            </div>
                        </li>
                        <br/>


                            <a href="pdf/control_gen.pdf" target="_blank">
                                <b>Posterior Control of Blackbox Generation</b>
                            </a>
                            <br/>
                            <b>Xiang Lisa Li</b>
                            and <a href="http://rush-nlp.com" target="_blank">Alexander Rush</a>

                            <br/>
                            In <a href="https://acl2020.org" target="_blank">
                                <b>
                                    Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics (ACL) </b></a>, 2020.
                            <br/>

                

                            [<a href="#" onclick="$('#acl2020_bib').toggle();return false;">bib</a>]
                            [<a href="#" onclick="$('#acl2020_abstract').toggle();return false;">abstract</a>]
                            [<a href="pdf/control_gen_app.pdf" target="_blank">appendix</a>]
                            <!-- [<a href="" target="_blank">dataset</a>] -->
                            <div id="acl2020_abstract" class="abstract" style="display:none;">
                                <p>
                                    Text generation often requires high-precision output that obeys task-specific rules. This fine-grained control is difficult to enforce with off-the-shelf deep learning models. In this work, we consider augmenting neural generation models with discrete control states learned through a structured latent-variable approach. Under this formulation, task-specific knowledge can be encoded through a range of rich, posterior constraints that are effectively trained into the model. This approach allows users to ground internal model decisions based on prior knowledge, without sacrificing the representational power of neural generative models. Experiments consider applications of this approach for text generation. We find that this method improves over standard benchmarks, while also providing fine-grained control.
                                </p>
                            </div>
                            <div id="acl2020_bib" class="bib" style="display:none;">
                                <pre>
                                @inproceedings{li-rush-2020,
                                  author =      {Xiang Lisa Li and Alexander M. Rush},
                                  title =       {Posterior Control of Blackbox Generation},
                                  booktitle =   {Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
                                  year =        {2020},
                                  month =       jul,
                                  address =     {Online},
                                  url =         {<a href="https://xiangli1999.github.io/pdf/control_gen.pdf">https://xiangli1999.github.io/pdf/control_gen.pdf</a>}
                                }
                                </pre>
                            </div>
                        </li>
                        <br/>



                        <li>
                            <a href="pdf/VIB.pdf" target="_blank">
                                <b>Specializing Word Embeddings (for Parsing) by Information Bottleneck</b>
                            </a>
                            <br/>
                            <b>Xiang Lisa Li</b>
                            and <a href="http://www.cs.jhu.edu/~jason/" target="_blank">Jason Eisner</a>
                            <br/>
                            In <a href="https://www.emnlp-ijcnlp2019.org" target="_blank">
                                <b>
                                    Conference on Empirical Methods in Natural Language Processing
                                    (EMNLP-IJCNLP)</b></a>, 2019.
                            	<br/>

                            <b>Best Paper Award at EMNLP-IJCNLP 2019</b> 
                            <br/>


                            [<a href="#" onclick="$('#emnlp2019_bib').toggle();return false;">bib</a>]
                            [<a href="#" onclick="$('#emnlp2019_abstract').toggle();return false;">abstract</a>]
                            [<a href="pdf/VIB-supp.pdf" target="_blank">appendix</a>]
                            <!-- [<a href="" target="_blank">dataset</a>] -->
                            <div id="emnlp2019_abstract" class="abstract" style="display:none;">
                                <p>
                                    Pre-trained word embeddings like ELMo and BERT contain rich syntactic 
  									and semantic information, resulting in state-of-the-art performance on 
  									various tasks.  We propose a very fast variational information bottleneck
  									(VIB) method to nonlinearly compress these embeddings, keeping only the
  									information that helps a discriminative parser. We compress each word
  									embedding to either a discrete tag or a continuous vector. 
  									In the discrete version, our automatically compressed tags form an 
  									alternative tag set: we show experimentally that our tags capture most 
  									of the information in traditional POS tag annotations, but our tag 
  									sequences can be parsed more accurately at the same level of tag granularity.
   									In the continuous version, we show experimentally that moderately compressing 
   									the word embeddings by our method yields a more accurate parser in 8 of 9 
   									languages, unlike simple dimensionality reduction.
                                </p>
                            </div>
                            <div id="emnlp2019_bib" class="bib" style="display:none;">
                            	<pre>
								@inproceedings{li-eisner-2019,
								  author =      {Xiang Lisa Li and Jason Eisner},
								  title =       {Specializing Word Embeddings (for Parsing) by
								                 Information Bottleneck},
								  booktitle =   {Proceedings of the 2019 Conference on Empirical
								                 Methods in Natural Language Processing and 9th
								                 International Joint Conference on Natural Language
								                 Processing},
								  year =        {2019},
								  month =       nov,
								  address =     {Hong Kong},
								  url =         {<a href="http://cs.jhu.edu/~jason/papers/#li-eisner-2019">http://cs.jhu.edu/~jason/papers/#li-eisner-2019</a>}
								}
								</pre>
                            </div>
                        </li>
                        <br/>
                        <li>
                            <a href="pdf/punctuation.pdf" target="_blank">
                                <b>A Generative Model for Punctuation in Dependency Trees</b>
                            </a>
                            <br/>
                            <b>Xiang Lisa Li </b> and
                            <a href="http://www.cs.jhu.edu/~wdd/" target="_blank">Dingquan Wang</a> and
                            <a href="http://www.cs.jhu.edu/~jason/" target="_blank">Jason Eisner</a>.
                            <br/>
                            In <a href="https://www.transacl.org/ojs/index.php/tacl" target="_blank">
                                <b> Transactions of the Association for Computational
                 					Linguistics (TACL)</b></a>, 2019.
                            <br/>
                            [<a href="#" onclick="$('#tacl2019_bib').toggle();return false;">bib</a>]
                            [<a href="#" onclick="$('#tacl2019_abstract').toggle();return false;">abstract</a>]
                            [<a href="https://arxiv.org/abs/1906.11298" target="_blank">arxiv</a>]

                            <div id="tacl2019_abstract" class="abstract" style="display:none;">
                                <p>
                                    Treebanks traditionally treat punctuation marks as ordinary words, but linguists have suggested that a tree’s “true” punctuation marks are not observed (Nunberg, 1990). These latent “underlying” marks serve to delimit or separate constituents in the syntax tree. When the tree’s yield is rendered as a written sentence, a string rewriting mechanism transduces the underlying marks into “surface” marks, which are part of the observed (surface) string but should not be regarded as part of the tree. We formalize this idea in a generative model of punctuation that admits efficient dynamic programming. We train it without observing the underlying marks, by locally maximizing the incomplete data likelihood (similarly to EM). When we use the trained model to reconstruct the tree’s underlying punctuation, the results appear plausible across 5 languages, and in particular, are consistent with Nunberg’s analysis of English. We show that our generative model can be used to beat baselines on punctuation restoration. Also, our reconstruction of a sentence’s underlying punctuation lets us appropriately render the surface punctuation (via our trained underlying-to-surface mechanism) when we syntactically transform the sentence.
                                </p>
                            </div>

                            <div id="tacl2019_bib" class="bib" style="display:none;">
                            	<pre>
								@inproceedings{li-eisner-2019,
								  author =      {Xiang Lisa Li and Jason Eisner},
								  title =       {Specializing Word Embeddings (for Parsing) by
								                 Information Bottleneck},
								  booktitle =   {Proceedings of the 2019 Conference on Empirical
								                 Methods in Natural Language Processing and 9th
								                 International Joint Conference on Natural Language
								                 Processing},
								  year =        {2019},
								  month =       nov,
								  address =     {Hong Kong},
								  url =         {<a href="http://cs.jhu.edu/~jason/papers/#li-eisner-2019">http://cs.jhu.edu/~jason/papers/#li-eisner-2019</a>}
								}
								</pre>
                            </div>

                        </li>
                    </ul>
<br/>
 <h5>Older publications on Medical Imaging:</h5>
                    <ul>
                        <li>
                            <a href="http://dx.doi.org/10.1007/978-3-030-02628-8_9" target="_blank">
                                <b>Shortcomings of Ventricle Segmentation Using Deep Convolutional Networks</b>
                            </a>
                            <br/>
                            Muhan Shao, Shuo Han, Aaron Carass, <b>Xiang Li</b>, Ari M. Blitz, Jerry L. Prince, Lotta M. Ellingsen 
                            <br/>
                            In 
                                <b> Deep Learning Fails Workshop in conjunction with the 21st International Conference on MICCAI</b>, 2018.
                            <br/>
                            
                        </li>
                        <br/>

                        <li>
                            <a href="http://dx.doi.org/10.1117/12.2295613" target="_blank">
                                <b>Multi-atlas Segmentation of the Hydrocephalus Brain Using an Adaptive Ventricle Atlas</b>
                            </a>
                            <br/>
                            Muhan Shao, Aaron Carass, <b>Xiang Li</b>, Blake E. Dewey, Ari M. Blitz, Jerry L. Prince, Lotta M. Ellingsen
                            <br/>
                            In 
                                <b>Proceedings of SPIE Medical Imaging</b>, 2018.
                            <br/>
                        </li>
                        <br/>

                        <li>
                            <a href="http://dx.doi.org/10.1117/12.2293633" target="_blank">
                                <b>Deformable Model Reconstruction of the Subarachnoid Space</b>
                            </a>
                            <br/>
                            Jeffrey Glaister, Muhan Shao, <b>Xiang Li</b>, Aaron Carass, Snehashis Roy, Ari M. Blitz, Jerry L. Prince, Lotta M. Ellingsen
                            <br/>
                            In 
                                <b>Proceedings of SPIE Medical Imaging</b>, 2018.
                            <br/>
                        </li>
                        <br/>

                        <li>
                            <a href="http://dx.doi.org/10.1007/978-3-319-67434-6_3" target="_blank">
                                <b>Whole Brain Parcellation with Pathology: Validation on Ventriculomegaly Patients</b>
                            </a>
                            <br/>
                            Aaron Carass, Muhan Shao, <b>Xiang Li</b>, Blake E. Dewey, Ari M. Blitz, Snehashis Roy, Dzung L. Pham, Jerry L. Prince, Lotta M. Ellingsen
                            <br/>
                            In 
                                <b>Workshop on Patch-based Techniques in Medical Imaging, MICCAI</b>, 2017.
                            <br/>
                        </li>



                    </ul>
                </div>
            </div>
            			<hr>
            <div class="row">
                <div class="col">
                    <h2>Honors & Awards</h2>
                    <ul>
                        <li>
                            (Sep. 2020)
                            Stanford Graduate Fellowship
                            <br/>
                        </li>

                        <li>
                            (May. 2020)
                            Outstanding Senior Award 
                            <br/>
                        </li>

                        <li>
                            (Dec. 2019)
                            <a href="https://cra.org/about/awards/outstanding-undergraduate-researcher-award/" target="_blank"> Outstanding Undergraduate Researcher Award  </a> (Computing Research Association) 
                        </li>
                        <li>
                            (Nov. 2019)
                            Best Paper Award at EMNLP-IJCNLP
                            <br/>
                        </li>

                    	<li>
                    		(Aug. 2019)
                    		<a href="https://www.cs.jhu.edu/masson-fellowship/" target="_blank">Gerald M. Masson Fellowship</a>
                            <br/>
                        </li>
                    	<li>
                            (May. 2019) 
                            <a href="https://www.cs.jhu.edu/2019/05/14/2019-convocation-and-department-awardees/#.XWYJ7pMzZ0I" target="_blank">
                            Michael J.Muuss Research Award </a>
                            <br/>	
                        </li>
                        <li>
                        	(April. 2019) 
                        	<a href="https://twitter.com/DCDataFest/status/1115054593740812295" target="_blank">
                           	Best Insight AND Best Visualization AND Best Use of Outside Data Award</a>

                       	</li>
                        <li>
                        	(Nov. 2018) 
                        	<a href="https://research.jhu.edu/hour/internal/pura/" target="_blank">
                           	Provost’s Undergraduate Research Award (PURA)</a>
                        </li>
                        <li>
                        	(May. 2018) 
                           	Research Experience for Undergraduate (REU) Fellowship
                        </li>
                        <li>
                        	(May. 2018) Fellowship William Huggins Summer Fellowship 
                        </li>
                        <li>
                        	(May. 2017) Summer Training and Research (STAR) Fellowship
                   		</li>
                   		<li>
                        	(Nov. 2018 - Present) Member of Tau Beta Pi
                   		</li>
                   		 <li>
                        	(April 2019 - Present) Member of Upsilon Pi Epsilon	
                   		</li>
                   		<li>
                        	(Sep. 2016 - Present) Dean's List
                   		</li>
                    </ul>
                </div>
            </div>

            <hr>
             <div class="row">
                <div class="col">
                    <h2>Teaching Experience</h2>
                    <ul>
                        <li>
                            (Spring 2020) TA @ Introduction to Statistics (AMS 553.430/630)
                        </li>
                    	<li>
                            (Fall 2019) CA @ Natural Language Processing (CS 520.465/665)
                        </li>
                    	<li>
                            (Spring 2019) TA @ Introduction to Probability (AMS 553.420/620)
                        </li>
                        <li>
                        	(Fall 2018) CA @ Natural Language Processing (CS 520.465/665)
                       	</li>
                        <li>
                        	(Fall 2018) TA @ Introduction to Probability (AMS 553.420/620)
                        </li>
                        <li>
                        	(Spring 2017) TA @ Introduction to Probability (AMS 553.420/620)
                        </li>
                        <li>
                        	(Fall 2017) TA @ Introduction to Probability (AMS 553.420/620)
                        </li>

                        <small>
                        	Interestingly, a perpetual prob TA is switching to stats...  Hope we can have fun in 430 :)
                        </small>
                  
                    </ul>
                </div>
            </div>

            <footer class="pt-2 my-md-2 pt-md-2 border-top">
                <div class="row justify-content-center">
                    <div class="col-6 col-md text-left align-self-center">
                        <p class="h5 text-muted">
                            Lisa, 2019
                        </p>
                    </div>
                </div>
            </footer>
        </div>
	    <script>
            (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
             (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
                                     m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
                                    })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
            ga('create', 'UA-51640218-1', 'auto');
            ga('send', 'pageview');
        </script>
    </body>
</html>
